\documentclass{statement}

% Author:  Lorena Barba


\setromanfont[Mapping=tex-text,
                 SmallCapsFont={Palatino},
                 SmallCapsFeatures={Scale=0.85}]{Palatino}
\setsansfont[Mapping=tex-text,Scale=0.9]{Optima} 
\setmonofont[Mapping=tex-text,Scale=0.85]{Monaco}
\newfontfamily\titfont[Mapping=tex-text]{Source Sans Pro ExtraLight}
\newfontfamily\secfont[Mapping=tex-text]{Source Sans Pro Bold}
\newfontfamily\subsecfont[Mapping=tex-text]{Source Sans Pro}
\renewcommand{\captionlabelfont}{\bf\sffamily}
\lhead{}
\chead{}
\rhead{\titfont Terminologies for Reproducible Research}
\lfoot{}
\cfoot{\thepage}
\rfoot{}

%\usepackage[T1]{fontenc}
%\usepackage[sc]{mathpazo}   % option clash with wrapfig !!

% Define the color to use in links:
\definecolor{linkcol}{rgb}{0.278,0.541,0.459}%

\definecolor{sectcol}{rgb}{0.63,0.16,0.16}
\definecolor{palevioletred4}{rgb}{0.545,0.278,0.365}
\definecolor{gray40}{rgb}{0.40,0.40,0.40}
\definecolor{gray26}{rgb}{0.26,0.26,0.26}
\definecolor{olive}{rgb}{0.5,0.5,0.0}
\definecolor{gray78}{cmyk}{0,0,0,0.22}



\usepackage[
    xetex,
    pdftitle={Terminologies for Reproducible Research},
    pdfauthor={Lorena Barba},
    pdfpagemode={UseOutlines},
    pdfpagelayout={TwoColumnRight},
    bookmarks, bookmarksopen,bookmarksnumbered={True},
    pdfstartview={FitH},
    colorlinks, linkcolor={linkcol},citecolor={linkcol},urlcolor={linkcol}
    ]{hyperref}

%% Define a new style for the url package that will use a smaller font.
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
\makeatother
%% Now actually use the newly defined style.
\urlstyle{leo}

% this handles hanging indents for publications
\def\rrr#1\\{\par
\medskip\hbox{\vbox{\parindent=2em\hsize=6.12in
\hangindent=4em\hangafter=1#1}}}

\def\baselinestretch{1}
\setlength{\parindent}{0mm} \setlength{\parskip}{0.8em}

\newlength{\up}
\setlength{\up}{-4mm}

\newlength{\hup}
\setlength{\hup}{-2mm}

\sectionfont{\secfont \color{palevioletred4}\vspace{-2mm}}
\subsectionfont{\secfont\vspace{-4mm}}
\subsubsectionfont{\normalsize\mdseries\itshape\vspace{-4mm}} %\itshape
\paragraphfont{\bfseries}

\newfontfamily\notefont[Mapping=tex-text,Scale=1]{Optima}

% -----------------------------------------------------------------------------------------------------------
\begin{document}
% -----------------------------------------------------------------------------------------------------------

% reset page numbering to 1
\pagenumbering{arabic}
\renewcommand{\thepage} {\arabic{page}}

\thispagestyle{empty}

{\titfont \Huge Terminologies for Reproducible Research} 
\medskip

{\fontspec[Mapping=tex-text,Scale=0.8]{Source Sans Pro} Lorena A. Barba, the George Washington University, Washington D.C.}

\vspace{1cm}

\section*{Introduction}
\vspace{\up}

Reproducible research---by its many names---has come to be regarded as a key concern across disciplines and stakeholder groups. Funding agencies and journals, professional societies and even mass media are paying attention, often focusing on the so-called ``crisis'' of reproducibility. One big problem keeps coming up among those seeking to tackle the issue: different groups are using terminologies in utter contradiction with each other. 
In July 2017, over a dozen participants joined the \href{https://collegeville.github.io/repeto/ReproducibilityWorkshop2017.html}{Workshop on Reproducibility Taxonomies for Computing and Computational Science} at the National Science Foundation, to appraise the variety of terminologies.
My presentation at that event \cite[]{barba2017-repeto} condensed a catalog of uses of the recurrent terms \emph{reproduce} and \emph{replicate}, often meaning different things but sometimes interchangeable.

Looking at a broad sample of publications in different fields, we can classify their terminology via decision tree: they either, \emph{A}---make no distinction between the words \emph{reproduce} and \emph{replicate}, or \emph{B}---use them distinctly.  
If \emph{B}, then they are commonly divided in two camps. 
In a spectrum of concerns that starts at a minimum standard of ``same data$+$same methods$=$same results,'' to ``new data and/or new methods in an independent study$=$same findings,'' group $1$ calls the minimum standard \emph{reproduce}, while group $2$ calls it \emph{replicate}.
This direct swap of the two terms aggravates an already weighty issue.
By attempting to inventory the terminologies across disciplines, I hope that some patterns will emerge to help us resolve the contradictions.


\section*{Pioneers of reproducible research}
\vspace{\up}

The first appearance of the phrase ``reproducible research'' in a scholarly publication appears to be an invited paper presented at the 1992 meeting of the Society of Exploration Geophysics (SEG), from the group of Jon Claerbout at Stanford \cite[]{claerbout1992}.
Claerbout pioneered the use of computers in processing and filtering seismic exploration data \cite[]{wiki:claerbout}.
From at least 1990, he required his students' PhD theses to conform to a standard of reproducibility.
His idea of reproducible research was to leave finished work (an article or a thesis) in a state that allowed colleagues to reproduce the calculation, analysis and final figures by executing a single command.
The goal was to merge a publication with its underlying computational analysis.
They used an automation tool called \texttt{make}, which builds software from source code by reading through a \texttt{makefile}: a list of commands to be executed in sequence.
Their workflow combined a set of standardized commands (burn, build, view, clean), and a filing system for the research compendium associated with the paper (data sets, programs, scripts, parameter files, makefiles).
Explicitly, \cite{claerbout1992} provide a ``definition of reproducibility in computationally oriented research.''
The original SEG paper is somewhat dated, but the group presented an updated overview in \cite{schwabETal2000}.
Here, the authors emphasize the limitations of the traditional methods of scientific publication, especially for computational research. 
In their vision of reproducible research, readers should be able to rebuild published results ``using the author's underlying programs and raw data.''
Implicitly, they are advocating for open code and data.

At Stanford, statistics professor David Donoho learned of Claerbout's methods in the early 1990s, and began adopting (and later promoting) them. 
In \cite{buckheit_donoho1995}, the authors state that``reproducibility of experiments in seismic exploration requires having the complete software environment available in other laboratories and the full source code available for inspection, modification, and application under varied parameter settings.'' 
This is because the goal in this field is to produce an image of the earth's sub-surface, from the datasets of seismic exploration signals. The software generating the image is key to the final results. 
And here is where the authors write the often-quoted ``slogan'' of reproducible research (paraphrasing Claerbout): 
\begin{quote}
\emph{``An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.''}
\end{quote}

\cite{buckheit_donoho1995} include many technical details that seem dated now, so perhaps a better place to read about this group's thinking and practice is \cite{donohoETal2009}. 
This appears to be the first article to publicly state that reproducibility depends on open code and data. The authors define \emph{reproducible computational research} as that ``in which all details of computations---code and data---are made conveniently available to others.'' 
The authors used the now-recurrent charged language about a ``credibility crisis'' in computational science.
They worry that computation cannot claim to be the ``third branch'' of science because most computational results cannot be verified. 
In the two traditional branches, standards of practice already exist for managing the ubiquity of error: deductive science uses formal logic and the mathematical proof, while empirical science uses statistical hypothesis testing and detailed methods reporting. 
The authors also counter the view that reproducibility means re-implementing the research software from the ground up. 
Because, in that case, if the new results did not match, ``the only way we'd ever get to the bottom of such a discrepancy is if we both worked reproducibly and studied detailed differences between code and data.''

The pioneering efforts of Claerbout and Donoho influenced many others. But their concerns centered on research involving computational analysis of unique data (recordings of seismic waves). 
They did not deal with the situation where another researcher might collect \emph{new} data, re-doing a study or an experiment following an original design, then analyze this data to compare the final findings with the original. 
Situations like this arise in many empirical fields, where scientific findings must be confirmed by independent studies. 
\cite{pengETal2006} distinguish the term \emph{replication} for this scenario. They say:
``Scientific evidence is strengthened when important findings are \emph{replicated} by multiple independent investigators using independent data, analytical methods, laboratories, and instruments.'' 
But epidemiologic studies are expensive, time-consuming, and often impossible to replicate (e.g., when they rely on opportunistic data, say, from an infectious outbreak). 
``An attainable minimum standard is \emph{reproducibility}, which calls for data sets and software to be made available for verifying published findings and conducting alternative analyses,'' they say.

In the social sciences, Harvard professor Gary King, one of the most cited political scientists of this generation, also pioneered the reproducibility concerns. 
In \cite{king1995}, he writes: ``The replication standard holds that sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author.'' He uses the term \emph{replication} throughout, not making a distinction like we have in the Claerbout/Donoho/Peng writings on reproducible research. 
In fact, his paper uses ``replication'' 96 times, ``replicate'' 21 times, and ``reproduce'' (as verb) only twice. E.g., ``good science requires that we be able to reproduce existing numerical results\ldots''
A more recent work co-authored by King, \cite[]{lazerETal2014}, still does not include the words ``reproduce/reproducibility/reproducible,'' and only uses ``replicate.'' 
This appears to be the more common usage in his field, as a Google Scholar search with ``replication political science`` gives 339,000 results, and with ``reproducible political science'' gives just 59,700 results (checked 01/14/2017).


\section*{Conflicting terminologies}
\vspace{\up}

As mentioned above, Claerbout and Donoho's pioneering work influenced many others, who adopt their definition of reproducible research. 
For example, \cite{gentlemanETal2007}: ``By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper.''
\cite{vandewalleETal2009}: ``A research work is called reproducible if all information relevant to the work, including, but not limited to, text, data and code, is made available, such that an independent researcher can reproduce the results.''
\cite{leveque2009}: ``The idea of `reproducible research' in scientific computing is to archive and make publicly available all the codes used to create a paper's figures or tables, preferably in such a manner that readers can download the codes and run them to reproduce the results.''
The distinction with the idea of replication, introduced by Peng, also appears in other works. 
The \emph{Annals of Internal Medicine} issued a statement saying: ``Independent replication by independent scientists in independent settings provides the best assurance that a scientific finding is valid; however, the resources and time required for high-quality clinical studies makes literal replication of published studies a slow corrective to any errors in the original publication. However, scientists and journal editors can promote `reproducible research,' [which ensures] that independent scientists can reproduce published results by using the same procedures and data as the original investigators'' \cite[]{laineETal2007}. 
In an editorial, the \emph{Int. Journal of Forecasting} announces that it will publish \emph{replication studies}: works attempting to independently verify research findings ``under the same or very similar conditions'' \cite[]{hyndman2010}. 
\emph{Science} published a special issue on ``Data Replication \& Reproducibility'' in 2011, to which the introduction reads: ``Replication---the confirmation of results and conclusions from one study obtained independently in another---is considered the scientific gold standard'' \cite[]{jasnyETal2011}.
In Peng's own (widely cited) article in this special issue, he introduces the idea of a \emph{reproducibility spectrum}, in which reproducible research is a ``minimum standard for judging scientific claims when full independent replication of a study is not possible.''

On the basis of the multiple references cited above, here are concise definitions that convey the meanings of the Claerbout/Donoho/Peng convention:

\begin{description}
\item[Reproducible research:] Authors provide all the necessary data and the computer codes to run the analysis again, re-creating the results.
\item[Replication:] Arriving at the same scientific findings as another study, collecting new data (possibly with different methods) and completing new analyses.
\end{description}

Unfortunately, conflicting terminologies started to spread in recent years. 
It appears that the first work to completely swap the usage of \emph{reproducible} and \emph{replicable} is \cite{drummond2009}. 
This is a workshop contribution to a conference in the field of machine learning (meaning, it received light or no peer review). 
The author seems to arbitrarily swap existing terminology; he says: ``I have used the term replicability for what others have called reproducibility in our literature.''
He provides no more justification than ``I think it reasonable.''
Mark Liberman, Distinguished Professor of Linguistics at the University of Pennsylvania, analyzed the usage of terms in the literature, and referring to Drummond's paper he concluded: ``Since the technical term `reproducible research' has been in use since 1990, and the technical distinction between reproducible and replicable at least since 2006, we should reject [the] attempt to re-coin technical terms reproducible and replicable in senses that assign the terms to concepts nearly opposite to those used in the definitions by Claerbout, Peng and others.'' \cite[]{liberman2015}




% ---------------------------------------------------------- END 


% ----------------------------------------------  REFERENCES 

\bibliography{repro}
\bibliographystyle{jponew}





%%%%%%%%
\end{document}
