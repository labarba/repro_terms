\documentclass{statement}

% Author:  Lorena Barba


\setromanfont[Mapping=tex-text,
                 SmallCapsFont={Palatino},
                 SmallCapsFeatures={Scale=0.85}]{Palatino}
\setsansfont[Mapping=tex-text,Scale=0.9]{Optima} 
\setmonofont[Mapping=tex-text,Scale=0.85]{Monaco}
\newfontfamily\titfont[Mapping=tex-text]{Source Sans Pro ExtraLight}
\newfontfamily\secfont[Mapping=tex-text]{Source Sans Pro Bold}
\newfontfamily\subsecfont[Mapping=tex-text]{Source Sans Pro}
\renewcommand{\captionlabelfont}{\bf\sffamily}
\lhead{}
\chead{}
\rhead{\titfont Terminologies for Reproducible Research}
\lfoot{}
\cfoot{\thepage}
\rfoot{}

%\usepackage[T1]{fontenc}
%\usepackage[sc]{mathpazo}   % option clash with wrapfig !!

% Define the color to use in links:
\definecolor{linkcol}{rgb}{0.278,0.541,0.459}%

\definecolor{sectcol}{rgb}{0.63,0.16,0.16}
\definecolor{palevioletred4}{rgb}{0.545,0.278,0.365}
\definecolor{gray40}{rgb}{0.40,0.40,0.40}
\definecolor{gray26}{rgb}{0.26,0.26,0.26}
\definecolor{olive}{rgb}{0.5,0.5,0.0}
\definecolor{gray78}{cmyk}{0,0,0,0.22}



\usepackage[
    xetex,
    pdftitle={Terminologies for Reproducible Research},
    pdfauthor={Lorena Barba},
    pdfpagemode={UseOutlines},
    pdfpagelayout={TwoColumnRight},
    bookmarks, bookmarksopen,bookmarksnumbered={True},
    pdfstartview={FitH},
    colorlinks, linkcolor={linkcol},citecolor={linkcol},urlcolor={linkcol}
    ]{hyperref}

%% Define a new style for the url package that will use a smaller font.
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
\makeatother
%% Now actually use the newly defined style.
\urlstyle{leo}

% this handles hanging indents for publications
\def\rrr#1\\{\par
\medskip\hbox{\vbox{\parindent=2em\hsize=6.12in
\hangindent=4em\hangafter=1#1}}}

\def\baselinestretch{1}
\setlength{\parindent}{0mm} \setlength{\parskip}{0.8em}

\newlength{\up}
\setlength{\up}{-4mm}

\newlength{\hup}
\setlength{\hup}{-2mm}

\sectionfont{\secfont \color{palevioletred4}\vspace{-2mm}}
\subsectionfont{\secfont\vspace{-4mm}}
\subsubsectionfont{\normalsize\mdseries\itshape\vspace{-4mm}} %\itshape
\paragraphfont{\bfseries}

\newfontfamily\notefont[Mapping=tex-text,Scale=1]{Optima}

% -----------------------------------------------------------------------------------------------------------
\begin{document}
% -----------------------------------------------------------------------------------------------------------

% reset page numbering to 1
\pagenumbering{arabic}
\renewcommand{\thepage} {\arabic{page}}

\thispagestyle{empty}

{\titfont \Huge Terminologies for Reproducible Research} 
\medskip

{\fontspec[Mapping=tex-text,Scale=0.8]{Source Sans Pro} Lorena A. Barba, the George Washington University, Washington D.C.}

\vspace{1cm}

\section*{Introduction}
\vspace{\up}

Reproducible research---by its many names---has come to be regarded as a key concern across disciplines and stakeholder groups. Funding agencies and journals, professional societies and even mass media are paying attention, often focusing on the so-called ``crisis'' of reproducibility. One big problem keeps coming up among those seeking to tackle the issue: different groups are using terminologies in utter contradiction with each other. 
In July 2017, over a dozen participants joined the \href{https://collegeville.github.io/repeto/ReproducibilityWorkshop2017.html}{Workshop on Reproducibility Taxonomies for Computing and Computational Science} at the National Science Foundation, to appraise the variety of terminologies.
My presentation at that event \cite[]{barba2017-repeto} condensed a catalog of uses of the recurrent terms \emph{reproduce} and \emph{replicate}, often meaning different things but sometimes interchangeable.

Looking at a broad sample of publications in different fields, we can classify their terminology via decision tree: they either, \emph{A}---make no distinction between the words \emph{reproduce} and \emph{replicate}, or \emph{B}---use them distinctly.  
If \emph{B}, then they are commonly divided in two camps. 
In a spectrum of concerns that starts at a minimum standard of ``same data$+$same methods$=$same results,'' to ``new data and/or new methods in an independent study$=$same findings,'' group $1$ calls the minimum standard \emph{reproduce}, while group $2$ calls it \emph{replicate}.
This direct swap of the two terms aggravates an already weighty issue.
By attempting to inventory the terminologies across disciplines, I hope that some patterns will emerge to help us resolve the contradictions.


\section*{Pioneers of reproducible research}
\vspace{\up}

The first appearance of the phrase ``reproducible research'' in a scholarly publication appears to be an invited paper presented at the 1992 meeting of the Society of Exploration Geophysics (SEG), from the group of Jon Claerbout at Stanford \cite[]{claerbout1992}.
Claerbout pioneered the use of computers in processing and filtering seismic exploration data \cite[]{wiki:claerbout}.
From at least 1990, he required his students' PhD theses to conform to a standard of reproducibility.
His idea of reproducible research was to leave finished work (an article or a thesis) in a state that allowed colleagues to reproduce the calculation, analysis and final figures by executing a single command.
The goal was to merge a publication with its underlying computational analysis.
They used an automation tool called \texttt{make}, which builds software from source code by reading through a \texttt{makefile}: a list of commands to be executed in sequence.
Their workflow combined a set of standardized commands (burn, build, view, clean), and a filing system for the research compendium associated with the paper (data sets, programs, scripts, parameter files, makefiles).
Explicitly, \cite{claerbout1992} provide a ``definition of reproducibility in computationally oriented research.''
The original SEG paper is somewhat dated, but the group presented an updated overview in \cite{schwabETal2000}.
Here, the authors emphasize the limitations of the traditional methods of scientific publication, especially for computational research. 
In their vision of reproducible research, readers should be able to rebuild published results ``using the author's underlying programs and raw data.''
Implicitly, they are advocating for open code and data.

At Stanford, statistics professor David Donoho learned of Claerbout's methods in the early 1990s, and began adopting (and later promoting) them. 
In \cite{buckheit_donoho1995}, the authors state that``reproducibility of experiments in seismic exploration requires having the complete software environment available in other laboratories and the full source code available for inspection, modification, and application under varied parameter settings.'' 
This is because the goal in this field is to produce an image of the earth's sub-surface, from the datasets of seismic exploration signals. The software generating the image is key to the final results. 
And here is where the authors write the often-quoted ``slogan'' of reproducible research (paraphrasing Claerbout): 
\begin{quote}
\emph{``An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.''}
\end{quote}

\cite{buckheit_donoho1995} include many technical details that seem dated now, so perhaps a better place to read about this group's thinking and practice is \cite{donohoETal2009}. 
This appears to be the first article to publicly state that reproducibility depends on open code and data. The authors define \emph{reproducible computational research} as that ``in which all details of computations---code and data---are made conveniently available to others.'' 
The authors used the now-recurrent charged language about a ``credibility crisis'' in computational science.
They worry that computation cannot claim to be the ``third branch'' of science because most computational results cannot be verified. 
In the two traditional branches, standards of practice already exist for managing the ubiquity of error: deductive science uses formal logic and the mathematical proof, while empirical science uses statistical hypothesis testing and detailed methods reporting. 
The authors also counter the view that reproducibility means re-implementing the research software from the ground up. 
Because, in that case, if the new results did not match, ``the only way we'd ever get to the bottom of such a discrepancy is if we both worked reproducibly and studied detailed differences between code and data.''

The pioneering efforts of Claerbout and Donoho influenced many others. But their concerns centered on research involving computational analysis of unique data (recordings of seismic waves). 
They did not deal with the situation where another researcher might collect \emph{new} data, re-doing a study or an experiment following an original design, then analyze this data to compare the final findings with the original. 
Situations like this arise in many empirical fields, where scientific findings must be confirmed by independent studies. 
\cite{pengETal2006} distinguish the term \emph{replication} for this scenario. They say:
``Scientific evidence is strengthened when important findings are \emph{replicated} by multiple independent investigators using independent data, analytical methods, laboratories, and instruments.'' 
But epidemiologic studies are expensive, time-consuming, and often impossible to replicate (e.g., when they rely on opportunistic data, say, from an infectious outbreak). 
``An attainable minimum standard is \emph{reproducibility}, which calls for data sets and software to be made available for verifying published findings and conducting alternative analyses,'' they say.



% ---------------------------------------------------------- END 


% ----------------------------------------------  REFERENCES 

\bibliography{repro}
\bibliographystyle{jponew}





%%%%%%%%
\end{document}
